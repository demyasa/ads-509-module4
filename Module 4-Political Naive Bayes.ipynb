{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes on Political Text\n",
    "\n",
    "In this notebook we use Naive Bayes to explore and classify political data. See the `README.md` for full details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Additional libraries\n",
    "import re \n",
    "from string import punctuation\n",
    "import pprint\n",
    "import spacy\n",
    "\n",
    "punctuation = set(punctuation) # speeds up comparison\n",
    "tw_punct = punctuation - {\"#\"}\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Feel free to include your text patterns functions\n",
    "\n",
    "def remove_stop(tokens) :\n",
    "    # modify this function to remove stopwords\n",
    "    stop_words = spacy.lang.en.STOP_WORDS                        # load stop words\n",
    "\n",
    "    potential_stop_words = [ '', 'im', 'like',                   \n",
    "                            'dont', 'got', 'cause',              # added extra stop words\n",
    "                            'wanna', 'youre']\n",
    "    \n",
    "    for wrd in potential_stop_words:\n",
    "        stop_words.add(wrd)\n",
    "\n",
    "    removed = [w for w in tokens if not w in stop_words]         # remove stop words\n",
    "    return(removed)\n",
    "\n",
    "def remove_url(text):\n",
    "    return re.sub(r'http\\S+', '', text)\n",
    "\n",
    "def remove_punctuation_keep_hash(text, punct_set=tw_punct) : \n",
    "    return(\"\".join([ch for ch in text if ch not in punct_set]))\n",
    " \n",
    "def remove_punctuation(text, punct_set=punctuation) : \n",
    "    return(\"\".join([ch for ch in text if ch not in punct_set]))\n",
    "\n",
    "def tokenize(text) : \n",
    "    \"\"\" Splitting on whitespace rather than the book's tokenize function. That \n",
    "        function will drop tokens like '#hashtag' or '2A', which we need for Twitter. \"\"\"\n",
    "    \n",
    "    # modify this function to return tokens\n",
    "    collapse_whitespace = re.compile(r'\\s+')\n",
    "    return([item.lower() for item in collapse_whitespace.split(text)])  # using Module 2\n",
    "    \n",
    "def prepare(text, pipeline) : \n",
    "    tokens = str(text)\n",
    "    \n",
    "    for transform in pipeline : \n",
    "        tokens = transform(tokens)\n",
    "        \n",
    "    return(tokens)\n",
    "\n",
    "def join_by_space(text_list):\n",
    "    return ' '.join(text_list)\n",
    "#from text_functions_solutions import clean_tokenize, get_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "convention_db = sqlite3.connect(\"2020_Conventions.db\")\n",
    "convention_cur = convention_db.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Exploratory Naive Bayes\n",
    "\n",
    "We'll first build a NB model on the convention data itself, as a way to understand what words distinguish between the two parties. This is analogous to what we did in the \"Comparing Groups\" class work. First, pull in the text \n",
    "for each party and prepare it for use in Naive Bayes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test query results\n",
    "\n",
    "for row in convention_cur.execute(\"SELECT party, text FROM conventions\"):\n",
    "    # print(row)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['party', 'text']\n"
     ]
    }
   ],
   "source": [
    "# Get column names from the convention_db\n",
    "\n",
    "names = [description[0] for description in convention_cur.description]\n",
    "print(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "convention_data = []\n",
    "\n",
    "# fill this list up with items that are themselves lists. The \n",
    "# first element in the sublist should be the cleaned and tokenized\n",
    "# text in a single string. The second element should be the party. \n",
    "\n",
    "query_results = convention_cur.execute(\n",
    "                            '''\n",
    "                            SELECT party, text FROM conventions\n",
    "                            ''')\n",
    "\n",
    "my_pipeline = [str.lower, remove_punctuation, tokenize, remove_stop, join_by_space]\n",
    "\n",
    "for row in query_results :\n",
    "    party = row[0]\n",
    "    raw_string = row[1]\n",
    "\n",
    "    # clean and tokenize the raw text string\n",
    "    clean_string = prepare(raw_string, my_pipeline)\n",
    "\n",
    "    # store the results in convention_data\n",
    "    convention_data.append([clean_string, party])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some random entries and see if they look right. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['need numbers overwhelming trump canâ€™t sneak steal way victory text vote 30330 started years ago yesterday 19th amendment constitution ratified took seven decades suffragists marching picketing going jail push closer perfect union 55 years ago john lewis marched bled selma work unfinished tonight iâ€™m thinking girls boys americaâ€™s future kamala harris black woman daughter jamaican indian immigrants nominee vice president countryâ€™s story breaking barriers expanding circle possibility',\n",
       "  'Democratic'],\n",
       " ['weâ€™ve 1864 trade tariffs china theyâ€™ve horrible language trade deal said china buy price advantage kind trade deal thatâ€™s deal covid hit plummeted getting thatâ€™s problem thatâ€™s day day battle',\n",
       "  'Democratic'],\n",
       " ['systematic decision countryâ€™s powerful leaders sell america china',\n",
       "  'Republican'],\n",
       " ['need experienced leader tony 3631 leader passion integrity strategic leadership skills chris f 3636 joe biden joseph w 3637 joe biden',\n",
       "  'Democratic'],\n",
       " ['want strong america want joe biden', 'Democratic'],\n",
       " ['heâ€™s longer beau inspires day beau served nation uniform year iraq decorated iraqi war veteran personally profound responsibility serving commander chief',\n",
       "  'Democratic'],\n",
       " ['montana', 'Republican'],\n",
       " ['thank good luck company soon youâ€™ll hundreds employees think right',\n",
       "  'Republican'],\n",
       " ['justice strength liberty', 'Democratic'],\n",
       " ['empathy thatâ€™s iâ€™ve thinking lot lately ability walk elseâ€™s shoes recognition elseâ€™s experience value practice second thought suffering struggling donâ€™t stand judgment reach grace god hard concept grasp itâ€™s teach children barack tried best instill girls strong moral foundation carry forward values parents grandparents poured',\n",
       "  'Democratic']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choices(convention_data,k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If that looks good, we now need to make our function to turn these into features. In my solution, I wanted to keep the number of features reasonable, so I only used words that occur at least `word_cutoff` times. Here's the code to test that if you want it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With a word cutoff of 5, we have 2278 as features in the model.\n"
     ]
    }
   ],
   "source": [
    "word_cutoff = 5\n",
    "\n",
    "tokens = [w for t, p in convention_data for w in t.split()]\n",
    "\n",
    "word_dist = nltk.FreqDist(tokens)\n",
    "\n",
    "feature_words = set()\n",
    "\n",
    "for word, count in word_dist.items() :\n",
    "    if count > word_cutoff :\n",
    "        feature_words.add(word)\n",
    "        \n",
    "print(f\"With a word cutoff of {word_cutoff}, we have {len(feature_words)} as features in the model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_features(text,fw) :\n",
    "    \"\"\"Given some text, this returns a dictionary holding the\n",
    "       feature words.\n",
    "       \n",
    "       Args: \n",
    "            * text: a piece of text in a continuous string. Assumes\n",
    "            text has been cleaned and case folded.\n",
    "            * fw: the *feature words* that we're considering. A word \n",
    "            in `text` must be in fw in order to be returned. This \n",
    "            prevents us from considering very rarely occurring words.\n",
    "        \n",
    "       Returns: \n",
    "            A dictionary with the words in `text` that appear in `fw`. \n",
    "            Words are only counted once. \n",
    "            If `text` were \"quick quick brown fox\" and `fw` = {'quick','fox','jumps'},\n",
    "            then this would return a dictionary of \n",
    "            {'quick' : True,\n",
    "             'fox' :    True}\n",
    "    \"\"\"\n",
    "    \n",
    "    ret_dict = dict()\n",
    "    text_list = text.split(' ')                     # split text by whitespace\n",
    "\n",
    "    for word in text_list:                          # iterate through text_list\n",
    "        if (word in fw) and (word not in ret_dict): # check for match in fw and one occurence\n",
    "            ret_dict[word] = True                   # add to ret_dict if True\n",
    "    \n",
    "    return(ret_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(feature_words)>0)\n",
    "assert(conv_features(\"donald is the president\",feature_words)==\n",
    "       {'donald':True,'president':True})\n",
    "assert(conv_features(\"people are american in america\",feature_words)==\n",
    "                     {'america':True,'american':True,\"people\":True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll build our feature set. Out of curiosity I did a train/test split to see how accurate the classifier was, but we don't strictly need to since this analysis is exploratory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(conv_features(text,feature_words), party) for (text, party) in convention_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(20220507)\n",
    "random.shuffle(featuresets)\n",
    "\n",
    "test_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.52\n"
     ]
    }
   ],
   "source": [
    "test_set, train_set = featuresets[:test_size], featuresets[test_size:]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                   china = True           Republ : Democr =     25.8 : 1.0\n",
      "                   votes = True           Democr : Republ =     23.8 : 1.0\n",
      "             enforcement = True           Republ : Democr =     21.5 : 1.0\n",
      "                 destroy = True           Republ : Democr =     19.2 : 1.0\n",
      "                freedoms = True           Republ : Democr =     18.2 : 1.0\n",
      "                 climate = True           Democr : Republ =     17.8 : 1.0\n",
      "                supports = True           Republ : Democr =     17.1 : 1.0\n",
      "                   crime = True           Republ : Democr =     16.1 : 1.0\n",
      "                   media = True           Republ : Democr =     14.9 : 1.0\n",
      "                 beliefs = True           Republ : Democr =     13.0 : 1.0\n",
      "               countries = True           Republ : Democr =     13.0 : 1.0\n",
      "                 defense = True           Republ : Democr =     13.0 : 1.0\n",
      "                    isis = True           Republ : Democr =     13.0 : 1.0\n",
      "                 liberal = True           Republ : Democr =     13.0 : 1.0\n",
      "                religion = True           Republ : Democr =     13.0 : 1.0\n",
      "                   trade = True           Republ : Democr =     12.7 : 1.0\n",
      "                    flag = True           Republ : Democr =     12.1 : 1.0\n",
      "               greatness = True           Republ : Democr =     12.1 : 1.0\n",
      "                 abraham = True           Republ : Democr =     11.9 : 1.0\n",
      "                  defund = True           Republ : Democr =     11.9 : 1.0\n",
      "                    drug = True           Republ : Democr =     10.9 : 1.0\n",
      "              department = True           Republ : Democr =     10.9 : 1.0\n",
      "               destroyed = True           Republ : Democr =     10.9 : 1.0\n",
      "                   enemy = True           Republ : Democr =     10.9 : 1.0\n",
      "               amendment = True           Republ : Democr =     10.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write a little prose here about what you see in the classifier. Anything odd or interesting?*\n",
    "\n",
    "### My Observations\n",
    "\n",
    "The results from the classifier displayed above allow us to gauge a few interesting observations about the 25 most informative words of the `feature_words` set. Based on the `nltk` documentation on the `show_most_informative_features` function: \n",
    "\n",
    "``\"the informativeness of a feature (fname,fval) is equal to the highest value of P(fname=fval|label), for any label, divided by the lowest value of P(fname=fval|label), for any label.\"``\n",
    "\n",
    "This means that the number ratio displayed to the right of the output represents words that tend to be used more frequently by a particular party. The two words that tend to be used most by individuals associated with the Democratic party are `\"votes\"` and `\"climate\"`, while the remaining 23 words fall under the rhetoric of individuals associated with the Republican party. The top three words most likely to be aligned with someone from the Republican party include `\"china\"`, `\"enforcement\"`, and `\"destroy\"`. These words appear in Republican texts approximately 20 times more than Democratic texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Classifying Congressional Tweets\n",
    "\n",
    "In this part we apply the classifer we just built to a set of tweets by people running for congress\n",
    "in 2018. These tweets are stored in the database `congressional_data.db`. That DB is funky, so I'll\n",
    "give you the query I used to pull out the tweets. Note that this DB has some big tables and \n",
    "is unindexed, so the query takes a minute or two to run on my machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cong_db = sqlite3.connect(\"congressional_data.db\")\n",
    "cong_cur = cong_db.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cong_cur.execute(\n",
    "        '''\n",
    "           SELECT DISTINCT \n",
    "                  cd.candidate, \n",
    "                  cd.party,\n",
    "                  tw.tweet_text\n",
    "           FROM candidate_data cd \n",
    "           INNER JOIN tweets tw ON cd.twitter_handle = tw.handle \n",
    "               AND cd.candidate == tw.candidate \n",
    "               AND cd.district == tw.district\n",
    "           WHERE cd.party in ('Republican','Democratic') \n",
    "               AND tw.tweet_text NOT LIKE '%RT%'\n",
    "        ''')\n",
    "\n",
    "results = list(results) # Just to store it, since the query is time consuming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['candidate', 'party', 'tweet_text']\n"
     ]
    }
   ],
   "source": [
    "tweet_data = []\n",
    "\n",
    "# Now fill up tweet_data with sublists like we did on the convention speeches.\n",
    "# Note that this may take a bit of time, since we have a lot of tweets.\n",
    "\n",
    "names = [description[0] for description in cong_cur.description]\n",
    "print(names)\n",
    "\n",
    "my_pipeline = [str.lower, remove_url,           # add remove url to pipeline\n",
    "               remove_punctuation_keep_hash,    # keep hash in punctuation\n",
    "               tokenize, remove_stop, join_by_space]\n",
    "\n",
    "for row in results :\n",
    "    party = row[1]\n",
    "    raw_string = row[2].decode(\"utf-8\")              # convert bytes to string\n",
    "    clean_string = prepare(raw_string, my_pipeline)  # clean raw string\n",
    "    tweet_data.append([clean_string, party])         # store tweet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of tweets here. Let's take a random sample and see how our classifer does. I'm guessing it won't be too great given the performance on the convention speeches..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['earlier today spoke house floor abt protecting health care women praised ppmarmonte work central coast', 'Democratic']\n",
      "['tribe #rallytogether', 'Democratic']\n",
      "['apparently trump thinks easy students overwhelmed crushing burden debt pay student loans #trumpbudget', 'Democratic']\n",
      "['weâ€™re grateful responders rescue personnel firefighters police volunteers working tirelessly people safe provide muchneeded help putting lives line', 'Republican']\n",
      "['letâ€™s greater #kag ðŸ‡ºðŸ‡¸', 'Republican']\n",
      "['1hr cavs tie series 22 #allin216 repbarbaralee scared #roadtovictory', 'Democratic']\n",
      "['congrats belliottsd new gig sd city hall glad continue serveâ€¦', 'Democratic']\n",
      "['close 3500 raised match right whoot thatâ€™s 7000 nonmath majors room ðŸ˜‚ help', 'Democratic']\n",
      "['today comment period potusâ€™s plan expand offshore drilling opened public 60 days march 9 share oppose proposed program directly trump administration comments email mail', 'Democratic']\n",
      "['celebrated icseastlaâ€™s 22 years eastside commitment amp saluted community leaders nightâ€™s awards dinner', 'Democratic']\n"
     ]
    }
   ],
   "source": [
    "random.seed(20201014)\n",
    "\n",
    "tweet_data_sample = random.choices(tweet_data,k=10)\n",
    "\n",
    "for row in tweet_data_sample:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's our (cleaned) tweet: earlier today spoke house floor abt protecting health care women praised ppmarmonte work central coast\n",
      "Actual party is Democratic and our classifer says Republican.\n",
      "\n",
      "Here's our (cleaned) tweet: tribe #rallytogether\n",
      "Actual party is Democratic and our classifer says Democratic.\n",
      "\n",
      "Here's our (cleaned) tweet: apparently trump thinks easy students overwhelmed crushing burden debt pay student loans #trumpbudget\n",
      "Actual party is Democratic and our classifer says Republican.\n",
      "\n",
      "Here's our (cleaned) tweet: weâ€™re grateful responders rescue personnel firefighters police volunteers working tirelessly people safe provide muchneeded help putting lives line\n",
      "Actual party is Republican and our classifer says Republican.\n",
      "\n",
      "Here's our (cleaned) tweet: letâ€™s greater #kag ðŸ‡ºðŸ‡¸\n",
      "Actual party is Republican and our classifer says Republican.\n",
      "\n",
      "Here's our (cleaned) tweet: 1hr cavs tie series 22 #allin216 repbarbaralee scared #roadtovictory\n",
      "Actual party is Democratic and our classifer says Republican.\n",
      "\n",
      "Here's our (cleaned) tweet: congrats belliottsd new gig sd city hall glad continue serveâ€¦\n",
      "Actual party is Democratic and our classifer says Republican.\n",
      "\n",
      "Here's our (cleaned) tweet: close 3500 raised match right whoot thatâ€™s 7000 nonmath majors room ðŸ˜‚ help\n",
      "Actual party is Democratic and our classifer says Republican.\n",
      "\n",
      "Here's our (cleaned) tweet: today comment period potusâ€™s plan expand offshore drilling opened public 60 days march 9 share oppose proposed program directly trump administration comments email mail\n",
      "Actual party is Democratic and our classifer says Republican.\n",
      "\n",
      "Here's our (cleaned) tweet: celebrated icseastlaâ€™s 22 years eastside commitment amp saluted community leaders nightâ€™s awards dinner\n",
      "Actual party is Democratic and our classifer says Republican.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# featuresets2 = [(conv_features(text,feature_words), party) for (text, party) in tweet_data_sample]\n",
    "\n",
    "for tweet, party in tweet_data_sample :\n",
    "    feature_set = conv_features(tweet, feature_words)   # generate feature set\n",
    "    estimated_party = classifier.classify(feature_set)  # predict party\n",
    "    \n",
    "    print(f\"Here's our (cleaned) tweet: {tweet}\")\n",
    "    print(f\"Actual party is {party} and our classifer says {estimated_party}.\")\n",
    "    print(\"\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've looked at it some, let's score a bunch and see how we're doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary of counts by actual party and estimated party. \n",
    "# first key is actual, second is estimated\n",
    "parties = ['Republican','Democratic']\n",
    "results = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for p in parties :\n",
    "    for p1 in parties :\n",
    "        results[p][p1] = 0\n",
    "\n",
    "\n",
    "num_to_score = 10000\n",
    "random.shuffle(tweet_data)\n",
    "\n",
    "for idx, tp in enumerate(tweet_data) :\n",
    "    tweet, party = tp    \n",
    "    # Now do the same thing as above, but we store the results rather\n",
    "    # than printing them. \n",
    "    feature_set = conv_features(tweet, feature_words)   # generate feature set\n",
    "    estimated_party = classifier.classify(feature_set)  # predict party\n",
    "    \n",
    "    results[party][estimated_party] += 1\n",
    "    \n",
    "    if idx > num_to_score : \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {'Republican': defaultdict(int,\n",
       "                         {'Republican': 3725, 'Democratic': 672}),\n",
       "             'Democratic': defaultdict(int,\n",
       "                         {'Republican': 4557, 'Democratic': 1048})})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflections\n",
    "\n",
    "_Write a little about what you see in the results_ \n",
    "\n",
    "Based on the dictionary output above we see the breakdown of the classifier's estimates on the tweet data. Based on the commentary of the dictionary created, the first keys represent the actual party associated with the tweet while the sub-dictionary contains the classifier's predictions. For all of the Republican party tweets, we can observe that the classifier predicted 3,725 tweets to be Republican and 672 to be Democratic. For all of the Democratic party tweets, we can see that 4,557 of them were classified as Republican and 1,048 were correctly predicted to be Democratic party tweets. From these results, it appears that the Naive Bayes Classifier that was trained is heavily skewed toward making Republican party classifications. Democratic classes were assigned 18.04% (672/3,725) of the time when the tweet was associated with Republicans, and only 23.00% (1,048/4,557) of the time when the tweet was associated with Democrats. Given that 23 of the top 25 most informative words were connected to Republican convention texts, it makes sense why this imbalance in predictions persists. Given the larger amount of tweet data than convention data, it might make sense to retrain the classifier on a portion of tweets as well. This may help the model generate more accurate predictions and the words considered to be most informative are likely to change as well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
